{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\nrequire(['notebook/js/codecell'], function(codecell) {\n  codecell.CodeCell.options_default.highlight_modes[\n      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n      Jupyter.notebook.get_cells().map(function(cell){\n          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n  });\n});\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pynq import Overlay, allocate\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Matmul_hw_32x8(Overlay):\n",
    "    def __init__(self, bitfile, **kwargs):\n",
    "        super().__init__(bitfile, **kwargs)\n",
    "        \n",
    "        self.matmul_in1 = self.axi_dma_0\n",
    "        self.matmul_in2 = self.axi_dma_1\n",
    "        self.matmul_out = self.axi_dma_2\n",
    "    \n",
    "    def matmul(self, in1, in2):\n",
    "        \n",
    "        in1_buffer = allocate(shape=(in1.shape[0],in1.shape[1]), dtype=\"float32\")\n",
    "        in2_buffer = allocate(shape=(in2.shape[0],in2.shape[1]), dtype=\"int8\")\n",
    "        out_buffer = allocate(shape=(in1.shape[0],in2.shape[1]), dtype=\"float32\")\n",
    "        \n",
    "        np.copyto(in1_buffer, in1)\n",
    "        np.copyto(in2_buffer, in2)\n",
    "        \n",
    "        self.matmul_in1.sendchannel.transfer(in1_buffer)\n",
    "        self.matmul_in2.sendchannel.transfer(in2_buffer)\n",
    "        self.matmul_out.recvchannel.transfer(out_buffer)\n",
    "        self.matmul_in1.sendchannel.wait()\n",
    "        self.matmul_in2.sendchannel.wait()\n",
    "        self.matmul_out.recvchannel.wait()\n",
    "        \n",
    "        return out_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hw_32x8 = Matmul_hw_32x8(\"matmul_32x8.bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "mnist = np.load(\"mnist-original.npy\", allow_pickle= True)\n",
    "\n",
    "x = mnist.item().get(\"data\").T / 255\n",
    "y = mnist.item().get(\"label\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is weight quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<weight quantized model, wl=1~8>\n",
      "wl=1\n",
      "Execution time: 122.88 seconds\n",
      "prediction accuracy is 13.14\n",
      " \n",
      "wl=2\n",
      "Execution time: 124.55 seconds\n",
      "prediction accuracy is 85.75\n",
      " \n",
      "wl=3\n",
      "Execution time: 122.17 seconds\n",
      "prediction accuracy is 96.61\n",
      " \n",
      "wl=4\n",
      "Execution time: 122.34 seconds\n",
      "prediction accuracy is 97.11\n",
      " \n",
      "wl=5\n",
      "Execution time: 120.73 seconds\n",
      "prediction accuracy is 97.11\n",
      " \n",
      "wl=6\n",
      "Execution time: 122.41 seconds\n",
      "prediction accuracy is 97.11\n",
      " \n",
      "wl=7\n",
      "Execution time: 120.79 seconds\n",
      "prediction accuracy is 97.11\n",
      " \n",
      "wl=8\n",
      "Execution time: 122.32 seconds\n",
      "prediction accuracy is 97.11\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"<weight quantized model, wl=1~8>\")\n",
    "def feed_forward_hw(X0):\n",
    "    X1 = np.matmul(X0, fc1w.T)\n",
    "    A1 = np.tanh(X1)\n",
    "    \n",
    "    HW_result = hw_32x8.matmul(A1, fc2w.T)\n",
    "    X2 = HW_result\n",
    "\n",
    "    A2 = np.tanh(X2)\n",
    "\n",
    "    X3 = np.matmul(A2, fc3w.T)\n",
    "    return X3\n",
    "\n",
    "\n",
    "for wl in range(1, 9):\n",
    "    file_name = f'quantized_model_wl={wl}.npy'\n",
    "    weights = np.load(file_name, allow_pickle=True)\n",
    "\n",
    "    fc1w = weights.item().get('fc1w')\n",
    "    # fc1b = weights.item().get('fc1b')\n",
    "\n",
    "    fc2w = weights.item().get('fc2w')\n",
    "    # fc2b = weights.item().get('fc2b')\n",
    "\n",
    "    fc3w = weights.item().get('fc3w')\n",
    "    # fc3b = weights.item().get('fc3b')\n",
    "\n",
    "    batch_size = 64\n",
    "\n",
    "    fc2w = fc2w.astype(\"int8\")\n",
    "    #print(f\"Data type: {fc2w.dtype}\")\n",
    "    \n",
    "    prediction = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for idx in range(len(x)//batch_size):\n",
    "        xs = x[batch_size * idx:batch_size * idx + batch_size]\n",
    "        # batch_size, 784\n",
    "        ys = y[batch_size * idx:batch_size*idx + batch_size]\n",
    "        # 1 dimension list : (batch_size,)\n",
    "        outputs = feed_forward_hw(xs) # (batch_size, 10)\n",
    "        for output, yk in zip(outputs, ys):\n",
    "            # zip() iterates over the outputs and ys lists in parallel.\n",
    "            prediction.append(np.argmax(output) == yk)\n",
    "            # argmax dim = 0 : coloumn direction (garo)\n",
    "        #print(prediction[idx])\n",
    "        \n",
    "    end_time = time.time()\n",
    "    \n",
    "    score = np.mean(prediction) * 100\n",
    "    \n",
    "    print(\"wl={}\".format(wl))\n",
    "    print(f\"Execution time: {end_time - start_time:.2f} seconds\")\n",
    "    print(\"prediction accuracy is {:.2f}\".format(score))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is not quantized any parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Matmul_hw_32x32(Overlay):\n",
    "    def __init__(self, bitfile, **kwargs):\n",
    "        super().__init__(bitfile, **kwargs)\n",
    "        \n",
    "        self.matmul_in1 = self.axi_dma_0\n",
    "        self.matmul_in2 = self.axi_dma_1\n",
    "        self.matmul_out = self.axi_dma_2\n",
    "    \n",
    "    def matmul(self, in1, in2):\n",
    "        \n",
    "        in1_buffer = allocate(shape=(in1.shape[0],in1.shape[1]), dtype=\"float32\")\n",
    "        in2_buffer = allocate(shape=(in2.shape[0],in2.shape[1]), dtype=\"float32\")\n",
    "        out_buffer = allocate(shape=(in1.shape[0],in2.shape[1]), dtype=\"float32\")\n",
    "        \n",
    "        np.copyto(in1_buffer, in1)\n",
    "        np.copyto(in2_buffer, in2)\n",
    "        \n",
    "        self.matmul_in1.sendchannel.transfer(in1_buffer)\n",
    "        self.matmul_in2.sendchannel.transfer(in2_buffer)\n",
    "        self.matmul_out.recvchannel.transfer(out_buffer)\n",
    "        self.matmul_in1.sendchannel.wait()\n",
    "        self.matmul_in2.sendchannel.wait()\n",
    "        self.matmul_out.recvchannel.wait()\n",
    "        \n",
    "        return out_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_32x32 = Matmul_hw_32x32(\"matmul_32x32.bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<no_qunatized_model>\n",
      "Execution time: 142.97 seconds\n",
      "Prediction accuracy is 97.58691674290942\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def feed_forward_hw_32x32(X0):\n",
    "    X1 = np.matmul(X0, fc1w.T)\n",
    "    A1 = np.tanh(X1)\n",
    "    \n",
    "    HW_result = hw_32x32.matmul(A1, fc2w.T)\n",
    "    X2 = HW_result\n",
    "\n",
    "    A2 = np.tanh(X2)\n",
    "\n",
    "    X3 = np.matmul(A2, fc3w.T)\n",
    "    return X3\n",
    "\n",
    "weights = np.load('float32_model.npy', allow_pickle=True)\n",
    "\n",
    "fc1w = weights.item().get('fc1w')\n",
    "fc2w = weights.item().get('fc2w')\n",
    "fc3w = weights.item().get('fc3w')\n",
    "\n",
    "batch_size = 64\n",
    "    \n",
    "prediction = []\n",
    "\n",
    "start_time = time.time()\n",
    "for idx in range(len(x)//batch_size):\n",
    "    xs = x[batch_size * idx:batch_size * idx + batch_size]\n",
    "    # batch_size, 784\n",
    "    ys = y[batch_size * idx:batch_size*idx + batch_size]\n",
    "    # 1 dimension list : (batch_size,)\n",
    "    outputs = feed_forward_hw_32x32(xs) # (batch_size, 10)\n",
    "    for output, yk in zip(outputs, ys):\n",
    "        # zip() iterates over the outputs and ys lists in parallel.\n",
    "        prediction.append(np.argmax(output) == yk)\n",
    "        # argmax dim = 0 : coloumn direction (garo)\n",
    "    #print(prediction[idx])\n",
    "    \n",
    "end_time = time.time()\n",
    "print(\"<no_qunatized_model>\")\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "score = np.mean(prediction) * 100\n",
    "\n",
    "print(\"Prediction accuracy is {}\".format(score))\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below is active and weight both quantized to int8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Matmul_hw_8x8(Overlay):\n",
    "    def __init__(self, bitfile, **kwargs):\n",
    "        super().__init__(bitfile, **kwargs)\n",
    "        \n",
    "        self.matmul_in1 = self.axi_dma_0\n",
    "        self.matmul_in2 = self.axi_dma_1\n",
    "        self.matmul_out = self.axi_dma_2\n",
    "    \n",
    "    def matmul(self, in1, in2):\n",
    "        \n",
    "        in1_buffer = allocate(shape=(in1.shape[0],in1.shape[1]), dtype=\"int8\")\n",
    "        in2_buffer = allocate(shape=(in2.shape[0],in2.shape[1]), dtype=\"int8\")\n",
    "        out_buffer = allocate(shape=(in1.shape[0],in2.shape[1]), dtype=\"int8\")\n",
    "        \n",
    "        np.copyto(in1_buffer, in1)\n",
    "        np.copyto(in2_buffer, in2)\n",
    "        \n",
    "        self.matmul_in1.sendchannel.transfer(in1_buffer)\n",
    "        self.matmul_in2.sendchannel.transfer(in2_buffer)\n",
    "        self.matmul_out.recvchannel.transfer(out_buffer)\n",
    "        self.matmul_in1.sendchannel.wait()\n",
    "        self.matmul_in2.sendchannel.wait()\n",
    "        self.matmul_out.recvchannel.wait()\n",
    "        \n",
    "        return out_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_8x8 = Matmul_hw_8x8(\"matmul_8x8.bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<both quantized model : 8bits active & weigth>\n",
      "Execution time: 109.99 seconds\n",
      "activation quantized prediction accuracy is 97.02\n"
     ]
    }
   ],
   "source": [
    "print(\"<both quantized model : 8bits active & weigth>\")\n",
    "\n",
    "weights = np.load('quantized_model_wl=8.npy',allow_pickle=True)\n",
    "\n",
    "fc1w = weights.item().get('fc1w')\n",
    "fc2w = weights.item().get('fc2w')\n",
    "fc3w = weights.item().get('fc3w')\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "fc2w = fc2w.astype(\"int8\")\n",
    "\n",
    "def fixed_point_quantize(x, wl, fl, clamp=True, symmetric=True):\n",
    "    scale = 2**(-fl)\n",
    "    if symmetric:\n",
    "        min_val = -2**(wl - fl - 1)\n",
    "        max_val = 2**(wl - fl - 1) - scale\n",
    "    else:\n",
    "        min_val = -2**(wl - fl - 1) + scale\n",
    "        max_val = 2**(wl - fl - 1) - scale\n",
    "    \n",
    "    if clamp:\n",
    "        x = np.clip(x, min_val, max_val)\n",
    "    \n",
    "    x_scaled = x / scale\n",
    "    x_rounded = np.round(x_scaled).astype(\"int8\")\n",
    "    return x_rounded\n",
    "\n",
    "def feed_forward_hw_8x8(X0):\n",
    "    X1 = np.matmul(X0, fc1w.T) \n",
    "    A1 = np.tanh(X1) \n",
    "\n",
    "    A1 = fixed_point_quantize(A1, wl=8, fl=4) # arbitary fl\n",
    "    #print(A1)\n",
    "    HW_result = hw_8x8.matmul(A1, fc2w.T) # int8 * int8\n",
    "    X2 = HW_result\n",
    "\n",
    "    A2 = np.tanh(X2)\n",
    "\n",
    "    X3 = np.matmul(A2, fc3w.T)\n",
    "    return X3\n",
    "\n",
    "prediction = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for idx in range(len(x)//batch_size):\n",
    "    xs = x[batch_size * idx:batch_size * idx + batch_size]\n",
    "    # batch_size, 784\n",
    "    ys = y[batch_size * idx:batch_size*idx + batch_size]\n",
    "    # 1 dimension list : (batch_size,)\n",
    "    outputs = feed_forward_hw_8x8(xs) # (batch_size, 10)\n",
    "    for output, yk in zip(outputs, ys):\n",
    "        # zip() iterates over the outputs and ys lists in parallel.\n",
    "        prediction.append(np.argmax(output) == yk)\n",
    "        # argmax dim = 0 : coloumn direction (garo)\n",
    "    #print(prediction[idx])\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "score = np.mean(prediction) * 100\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "print(\"activation quantized prediction accuracy is {:.2f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
